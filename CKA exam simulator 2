Solve this question on: ssh cka6016

The Deployment controller in Namespace lima-control communicates with various cluster internal endpoints by using their DNS FQDN values.

Update the ConfigMap used by the Deployment with the correct FQDN values for:

DNS_1: Service kubernetes in Namespace default
DNS_2: Headless Service department in Namespace lima-workload
DNS_3: Pod section100 in Namespace lima-workload. It should work even if the Pod IP changes
DNS_4: A Pod with IP 1.2.3.4 in Namespace kube-system
Ensure the Deployment works with the updated values.

ℹ️ You can use nslookup or dig inside a Pod of the controller Deployment
----------------------------------
Solve this question on: ssh cka2560

Create a Static Pod named my-static-pod in Namespace default on the controlplane node. It should be of image nginx:1-alpine and have resource requests for 10m CPU and 20Mi memory.

Create a NodePort Service named static-pod-service which exposes that static Pod on port 80.

ℹ️ For verification check if the new Service has one Endpoint. It should also be possible to access the Pod via the cka2560 internal IP address, like using curl 192.168.100.31:NODE_PORT
-------------------------------------
Solve this question on: ssh cka5248

Node cka5248-node1 has been added to the cluster using kubeadm and TLS bootstrapping.

Find the Issuer and Extended Key Usage values on cka5248-node1 for:

Kubelet Client Certificate, the one used for outgoing connections to the kube-apiserver
Kubelet Server Certificate, the one used for incoming connections from the kube-apiserver
Write the information into file /opt/course/3/certificate-info.txt.

ℹ️ You can connect to the worker node using ssh cka5248-node1 from cka5248
---------------------------------------
Solve this question on: ssh cka3200

Do the following in Namespace default:

Create a Pod named ready-if-service-ready of image nginx:1-alpine
Configure a LivenessProbe which simply executes command true
Configure a ReadinessProbe which does check if the url http://service-am-i-ready:80 is reachable, you can use wget -T2 -O- http://service-am-i-ready:80 for this
Start the Pod and confirm it isn't ready because of the ReadinessProbe.
Then:

Create a second Pod named am-i-ready of image nginx:1-alpine with label id: cross-server-ready
The already existing Service service-am-i-ready should now have that second Pod as endpoint
Now the first Pod should be in ready state, check that
------------------------------------
Solve this question on: ssh cka8448

Create two bash script files which use kubectl sorting to:

Write a command into /opt/course/5/find_pods.sh which lists all Pods in all Namespaces sorted by their AGE (metadata.creationTimestamp)

Write a command into /opt/course/5/find_pods_uid.sh which lists all Pods in all Namespaces sorted by field metadata.uid
------------------------------------
Solve this question on: ssh cka1024

There seems to be an issue with the kubelet on controlplane node cka1024, it's not running.

Fix the kubelet and confirm that the node is available in Ready state.

Create a Pod called success in default Namespace of image nginx:1-alpine.

ℹ️ The node has no taints and can schedule Pods without additional tolerations
-----------------------------------
Solve this question on: ssh cka2560

You have been tasked to perform the following etcd operations:

Run etcd --version and store the output at /opt/course/7/etcd-version
Make a snapshot of etcd and save it at /opt/course/7/etcd-snapshot.db
-----------------------------------
Solve this question on: ssh cka8448

Check how the controlplane components kubelet, kube-apiserver, kube-scheduler, kube-controller-manager and etcd are started/installed on the controlplane node.

Also find out the name of the DNS application and how it's started/installed in the cluster.

Write your findings into file /opt/course/8/controlplane-components.txt. The file should be structured like:

# /opt/course/8/controlplane-components.txt
kubelet: [TYPE]
kube-apiserver: [TYPE]
kube-scheduler: [TYPE]
kube-controller-manager: [TYPE]
etcd: [TYPE]
dns: [TYPE] [NAME]
Choices of [TYPE] are: not-installed, process, static-pod, pod
-------------------------------------
Solve this question on: ssh cka5248

Temporarily stop the kube-scheduler, this means in a way that you can start it again afterwards.

Create a single Pod named manual-schedule of image httpd:2-alpine, confirm it's created but not scheduled on any node.

Now you're the scheduler and have all its power, manually schedule that Pod on node cka5248. Make sure it's running.

Start the kube-scheduler again and confirm it's running correctly by creating a second Pod named manual-schedule2 of image httpd:2-alpine and check if it's running on cka5248-node1.
---------------------------------------
Solve this question on: ssh cka6016

There is a backup Job which needs to be adjusted to use a PVC to store backups.

Create a StorageClass named local-backup which uses provisioner: rancher.io/local-path and volumeBindingMode: WaitForFirstConsumer. To prevent possible data loss the StorageClass should keep a PV retained even if a bound PVC is deleted.

Adjust the Job at /opt/course/10/backup.yaml to use a PVC which request 50Mi storage and uses the new StorageClass.

Deploy your changes, verify the Job completed once and the PVC was bound to a newly created PV.

ℹ️ To re-run a Job, delete it and create it again

ℹ️ The abbreviation PV stands for PersistentVolume and PVC for PersistentVolumeClaim
---------------------------------------
Solve this question on: ssh cka2560

Create Namespace secret and implement the following in it:

Create Pod secret-pod with image busybox:1. It should be kept running by executing sleep 1d or something similar

Create the existing Secret /opt/course/11/secret1.yaml and mount it readonly into the Pod at /tmp/secret1

Create a new Secret called secret2 which should contain user=user1 and pass=1234. These entries should be available inside the Pod's container as environment variables APP_USER and APP_PASS
--------------------------------------
Solve this question on: ssh cka5248

Create a Pod of image httpd:2-alpine in Namespace default.

The Pod should be named pod1 and the container should be named pod1-container.

This Pod should only be scheduled on controlplane nodes.

Do not add new labels to any nodes.
--------------------------------------
Solve this question on: ssh cka3200

Create a Pod with multiple containers named multi-container-playground in Namespace default:

It should have a volume attached and mounted into each container. The volume shouldn't be persisted or shared with other Pods

Container c1 with image nginx:1-alpine should have the name of the node where its Pod is running on available as environment variable MY_NODE_NAME

Container c2 with image busybox:1 should write the output of the date command every second in the shared volume into file date.log. You can use while true; do date >> /your/vol/path/date.log; sleep 1; done for this.

Container c3 with image busybox:1 should constantly write the content of file date.log from the shared volume to stdout. You can use tail -f /your/vol/path/date.log for this.

ℹ️ Check the logs of container c3 to confirm correct setup


---------------------------------------
Solve this question on: ssh cka8448

You're ask to find out following information about the cluster:

How many controlplane nodes are available?
How many worker nodes (non controlplane nodes) are available?
What is the Service CIDR?
Which Networking (or CNI Plugin) is configured and where is its config file?
Which suffix will static pods have that run on cka8448?
Write your answers into file /opt/course/14/cluster-info, structured like this:

# /opt/course/14/cluster-info
1: [ANSWER]
2: [ANSWER]
3: [ANSWER]
4: [ANSWER]
5: [ANSWER]

---------------------------------------
Solve this question on: ssh cka6016

Write a kubectl command into /opt/course/15/cluster_events.sh which shows the latest events in the whole cluster, ordered by time (metadata.creationTimestamp)
Delete the kube-proxy Pod and write the events this caused into /opt/course/15/pod_kill.log on cka6016
Manually kill the containerd container of the kube-proxy Pod and write the events into /opt/course/15/container_kill.log
-----------------------------------------
Solve this question on: ssh cka3200

Write the names of all namespaced Kubernetes resources (like Pod, Secret, ConfigMap...) into /opt/course/16/resources.txt.

Find the project-* Namespace with the highest number of Roles defined in it and write its name and amount of Roles into /opt/course/16/crowded-namespace.txt.
--------------------------------------------
Solve this question on: ssh cka6016

There is Kustomize config available at /opt/course/17/operator. It installs an operator which works with different CRDs. It has been deployed like this:

kubectl kustomize /opt/course/17/operator/prod | kubectl apply -f -
Perform the following changes in the Kustomize base config:

The operator needs to list certain CRDs. Check the logs to find out which ones and adjust the permissions for Role operator-role
Add a new Student resource called student4 with any name and description
Deploy your Kustomize config changes to prod.
